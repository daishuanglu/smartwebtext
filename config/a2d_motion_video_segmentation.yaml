model_name: 'a2d_motion_video_image_segmentor'
network_type: 'motion_gan'
train_val_ratio: [0.95, 0.05]
skip_prep_data: True
dataset_dir: 'D:/video_datasets/A2D'
label_colors_json: 'D:/video_datasets/A2D/label_colors.json'
val_prediction_fstr: 'val_predictions/{vid}/{fids}.jpg'
#Model
skip_training: False
input_size: [320, 768]
#dense keypoint parameters
# Number of features mutliplier
block_expansion: 64
# Maximum allowed number of features
max_features: 1024
# Number of block in Unet.
num_blocks: 5
# pix2pix settings
patch_size: 256
gen_features: 64
n_gen_enc: 6
disc_features: [64,128, 256, 512]
in_channels: 3
l1_lambda: 100
l2_lambda: 100
# end of pix2pix settings
out_channels: 64 #total 64 action-actor classes in A2D dataset
features: [64, 128, 256, 512]
bilinear: True
#Settings
learning_rate: 0.0001
monitor: 'val_loss'
epochs: 50
batch_size: 2
clear_cache: False
limit:
logger_dir: 'lightning_logs'
resume_ckpt: True
# Defines model architecture
hourglass_gru_params:
  #block_expansion: 64
  # Maximum allowed number of features
  max_features: 512
  # Number of downsampling blocks and Upsampling blocks.
  num_blocks: 3
tps_model_params:
  common_params:
    # Number of TPS transformation
    num_tps: 10
    # Number of channels per image
    num_channels: 3
    # Whether to estimate affine background transformation
    bg: True
    # Whether to estimate the multi-resolution occlusion masks
    multi_mask: True
  generator_params:
    # Number of features mutliplier
    block_expansion: 64
    # Maximum allowed number of features
    max_features: 512
    # Number of downsampling blocks and Upsampling blocks.
    num_down_blocks: 3
  dense_motion_params:
    # Number of features mutliplier
    block_expansion: 64
    # Maximum allowed number of features
    max_features: 1024
    # Number of block in Unet.
    num_blocks: 5
    # Optical flow is predicted on smaller images for better performance,
    # scale_factor=0.25 means that 256x256 image will be resized to 64x64
    scale_factor: 0.25
  avd_network_params:
    # Bottleneck for identity branch
    id_bottle_size: 128
    # Bottleneck for pose branch
    pose_bottle_size: 128
# Parameters of training
tps_train_params:
  # Number of training epochs
  num_epochs: 100
  # For better i/o performance when number of videos is small number of epochs can be multiplied by this number.
  # Thus effectivlly with num_repeats=100 each epoch is 100 times larger.
  num_repeats: 150
  # Drop learning rate by 10 times after this epochs
  epoch_milestones: [70, 90]
  # Initial learing rate for all modules
  lr_generator: 2.0e-4
  batch_size: 28
  # Scales for perceptual pyramide loss. If scales = [1, 0.5, 0.25, 0.125] and image resolution is 256x256,
  # than the loss will be computer on resolutions 256x256, 128x128, 64x64, 32x32.
  scales: [1, 0.5, 0.25, 0.125]
  # Dataset preprocessing cpu workers
  dataloader_workers: 12
  # Save checkpoint this frequently. If checkpoint_freq=50, checkpoint will be saved every 50 epochs.
  checkpoint_freq: 50
  # Parameters of dropout
  # The first dropout_epoch training uses dropout operation
  dropout_epoch: 35
  # The probability P will linearly increase from dropout_startp to dropout_maxp in dropout_inc_epoch epochs
  dropout_maxp: 0.7
  dropout_startp: 0.0
  dropout_inc_epoch: 10
  # Estimate affine background transformation from the bg_start epoch.
  bg_start: 0
  # Parameters of random TPS transformation for equivariance loss
  transform_params:
    # Sigma for affine part
    sigma_affine: 0.05
    # Sigma for deformation part
    sigma_tps: 0.005
    # Number of point in the deformation grid
    points_tps: 5
  loss_weights:
    # Weights for perceptual loss.
    perceptual: [10, 10, 10, 10, 10]
    # Weights for value equivariance.
    equivariance_value: 10
    # Weights for warp loss.
    warp_loss: 10
    # Weights for bg loss.
    bg: 10
